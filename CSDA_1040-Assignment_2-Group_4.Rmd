---
title: 'CSDA 1040: Assignment 2 - Group 4'
author: Jose German, Anjana Pradeep Kumar, Anupama Radhakrishnan Kowsalya, and Xenel
  Nazar
date: "27/06/2020"
output: html_document
---

# 1.0 Abstract

# 2.0 Introduction

# 3.0 Objectives

# 4.0 Data Understanding

### 4.1 About the Data

The data includes twitter data that was scrapped from February 2015 detailing the problems of each major U.S. Airline and hosted on Kaggle (https://www.kaggle.com/crowdflower/twitter-airline-sentiment). The data was originally collected by Crowdflower (https://data.world/crowdflower/airline-twitter-sentiment) previously known as Figure Eight Inc., and subsequently acquired by Appen an AI data company (https://appen.com/). Contributors helped classify the tweets as either positive, negative, or neutral, and then categorizing neutral tweets under various negative reasons (e.g. "late flight", "rude service) (https://data.world/crowdflower/airline-twitter-sentiment). Prior to the initial load into Kaggle, certain transformations were done by Ben Hammer the Co-Founder and CTO at Kaggle. (https://github.com/benhamner/crowdflower-airline-twitter-sentiment/blob/master/src/process.py). 

https://data.world/crowdflower/airline-twitter-sentiment

https://www.kaggle.com/crowdflower/twitter-airline-sentiment/data

https://appen.com/

https://github.com/benhamner/crowdflower-airline-twitter-sentiment/blob/master/src/process.py

### 4.2 Import Data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
getwd()
```

```{r}
# Import Data
tweets=read.csv("Tweets.csv",na.strings=c("", "NA"),stringsAsFactors = FALSE)
```

### 4.3 Import Packages

```{r}
# Load Libraries
library(dplyr)
library(tidytext)
library(ggplot2)
library(stringr)
library(RColorBrewer)
library(wordcloud)
library(tm)
library(NLP)
library(SentimentAnalysis)
library(e1071)
```

# 5.0	Data Exploration and Preparation

Overview of Data
```{r}
# Summary of Data
str(tweets)
```

```{r}
# Details of Data
summary(tweets)
```

### 5.1 Initial Data cleanup

Remove columns we will not use.
```{r}
# Drop unused columns
tweets<- subset(tweets, select = -c(tweet_id,airline_sentiment_gold,negativereason_gold,tweet_coord) )
```

To simplify we will rename the columns
```{r}
# Rename columns
 new_colname<-c("sentiment","confidence","reason","negconfidence","airline","user","retweet","text","created","location","timezone")
colnames(tweets)<-new_colname
# View columns
colnames(tweets)
```

Convert the Created Date data from String to Datetime format
```{r}
# Convert created date from string to datetime
tweets$created<-as.Date(tweets$created)
```

```{r}
# Verify column data types
sapply(tweets,class)
```

Replace NAs from reason column
```{r}
# Replace NA with Unknown
tweets$reason[is.na(tweets$reason)]<-"Unknown"
```

Replace NAs under the negative reason confidence with mean 
```{r}
# Replace NA with mean
confMean=trunc(mean(tweets$negconfidence, na.rm = TRUE))
tweets$negconfidence[is.na(tweets$negconfidence)]<-confMean

```

Replace NAs under location, with Not Available
```{r}
# Replace NA with Not Available
tweets$location[is.na(tweets$location)]<-"Not Available"

```

Replace NAs under timezone with Not Available
```{r}
# Replace NA with not Available
tweets$timezone[is.na(tweets$timezone)]<-"Not Available"
```

Remove any other NAs in dataframe
```{r}
# Remove NA's if any
tweets<-na.omit(tweets)
```

Check data cleanup
```{r}
# Checking data cleanup
dim(tweets)
table(is.na(tweets))
```

### 5.2 Visualization of Data

Plot tweets by Airline
```{r}
# Plot tweets by Airline
tweetsbyAirline<-tweets%>%
  group_by(airline,created)%>%
  summarise(tcount=n())
tweetsbyAirlinePlot=ggplot()+geom_line(data=tweetsbyAirline,aes(x=created,y=tcount,group=airline,color=airline))
tweetsbyAirlinePlot
```
Plot tweets by Sentiment
```{r}
# Plot tweets by Sentiment
tweetsbySentiment<-tweets%>%
  group_by(sentiment,created)%>%
  summarise(scount=n())
tweetsbySentimentPlot=ggplot()+geom_line(data=tweetsbySentiment,aes(x=created,y=scount,group=sentiment,color=sentiment))
tweetsbySentimentPlot
```
```{r}
# Sentiment count by airline
ggplot(tweets, aes(x = sentiment, fill = sentiment)) +
  geom_bar() +
  facet_grid(. ~ airline) +
  theme(axis.text.x = element_text(angle=65, vjust=0.6),
        plot.margin = unit(c(3,0,3,0), "cm"))
```
```{r}
# Get airlines listed in tweets
allAirlines <- distinct(tweets, airline)
allAirlines <- lapply(allAirlines, as.character)
print(allAirlines)
```

Plot tweets by Timezone
```{r}
# Plot tweets by Timezone
tweetsbyTimezone<-tweets%>%
  group_by(timezone)%>%
  summarise(tzcount=n())%>%
  filter(tzcount>50)

tweetsbyTimezonePlot<-ggplot(data = tweetsbyTimezone) +
  geom_col(mapping = aes(x = tzcount, y = timezone))
tweetsbyTimezonePlot
```
Plot tweets by Negative Sentiment Reason
```{r}
# Plot tweets by Sentiment Reason
tweetsbySentimentreason<-tweets%>%
  filter(sentiment=="negative")%>%
  group_by(sentiment,reason)%>%
  summarise(srcount=n())
tweetsbySentimentreasonPlot<-ggplot(tweetsbySentimentreason) +
  geom_col(
    mapping = aes(x = sentiment, y = srcount, fill = reason), position = "dodge"
  )
tweetsbySentimentreasonPlot
```
### 5.3 Tweets text cleanup
```{r}
# Cleaning tweet
tweets$text <- str_replace_all(tweets$text,"@[a-z,A-Z]*","")  
tweets$text <- gsub("&amp", "", tweets$text)
tweets$text <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets$text)
tweets$text <- gsub("@\\w+", "", tweets$text)
tweets$text <- gsub("[[:punct:]]", "",tweets$text)
tweets$text <- gsub("[[:digit:]]", "", tweets$text)
tweets$text <- gsub("http\\w+", "",tweets$text)
tweets$text <- gsub("[ \t]{2,}", "",tweets$text)
tweets$text <-gsub("^\\s+|\\s+$", "", tweets$text) 
```

```{r}
# Cleaning tweet 2
#https://gist.github.com/CateGitau/05e6ff80b2a3aaa58236067811cee44e
tweets$text <- gsub("http[[:alnum:]]*",'', x)
tweets$text <- gsub('http\\S+\\s*', '', x) ## Remove URLs
tweets$text <- gsub('\\b+RT', '', x) ## Remove RT
tweets$text <- gsub('#\\S+', '', x) ## Remove Hashtags
tweets$text <- gsub('@\\S+', '', x) ## Remove Mentions
tweets$text <- gsub('[[:cntrl:]]', '', x) ## Remove Controls and special characters
tweets$text <- gsub("\\d", '', x) ## Remove Controls and special characters
tweets$text <- gsub('[[:punct:]]', '', x) ## Remove Punctuations
tweets$text <- gsub("^[[:space:]]*","",x) ## Remove leading whitespaces
tweets$text <- gsub("[[:space:]]*$","",x) ## Remove trailing whitespaces
tweets$text <- gsub(' +',' ',x) ## Remove extra whitespaces
```

### 5.4 Corpus Setup
Corpus setup (https://gist.github.com/CateGitau/05e6ff80b2a3aaa58236067811cee44e)
```{r}
# Build a corpus and specify the source to be character of vectors
tweetCorpus <- Corpus(VectorSource(tweets$text))

Textprocessing <- function(x)
  {gsub("http[[:alnum:]]*",'', x)
  gsub('http\\S+\\s*', '', x) ## Remove URLs
  gsub('\\b+RT', '', x) ## Remove RT
  gsub('#\\S+', '', x) ## Remove Hashtags
  gsub('@\\S+', '', x) ## Remove Mentions
  gsub('[[:cntrl:]]', '', x) ## Remove Controls and special characters
  gsub("\\d", '', x) ## Remove Controls and special characters
  gsub('[[:punct:]]', '', x) ## Remove Punctuations
  gsub("^[[:space:]]*","",x) ## Remove leading whitespaces
  gsub("[[:space:]]*$","",x) ## Remove trailing whitespaces
  gsub(' +',' ',x) ## Remove extra whitespaces
}
tweetCorpus <- tm_map(tweetCorpus, function(x) iconv(enc2utf8(x), sub = "byte"))

# Convert myCorpus into lowercase
tweetCorpus <- tm_map(tweetCorpus, content_transformer(tolower))
# Remove punctuation
tweetCorpus <- tm_map(tweetCorpus, removePunctuation)
# Remove numbers
tweetCorpus <- tm_map(tweetCorpus, removeNumbers)
```

Verify Corpus
```{r}
# Verify corpus
tweetCorpus[[8]]$content
```

Remove stopwords
```{r}
# Remove stopwords
tweetCorpus<-tm_map(tweetCorpus,removeWords,stopwords("english"))
# Remove stop word flight
customstopwords <- c("flight","airline","get","got","dont","will","ive","","told","day","still","can","cant")
tweetCorpus<-tm_map(tweetCorpus,removeWords,customstopwords)

tweetCorpus <- tm_map(tweetCorpus,Textprocessing)
```

```{r}
#Creating corpus
#tweetCorpus<-SimpleCorpus(VectorSource(tweets$text))
```
```{r}
#Cleaning Corpus
 

#Remove punctuation
#tweetCorpus<-tm_map(tweetCorpus,removePunctuation)
#Remove numbers
# tweetCorpus<-tm_map(tweetCorpus,removeNumbers)
  # To lower case
 #tweetCorpus<-tm_map(tweetCorpus,content_transformer(tolower))
#Remove white space
#tweetCorpus<-tm_map(tweetCorpus,stripWhitespace)
#Remove stopwords
#tweetCorpus<-tm_map(tweetCorpus,removeWords,stopwords("english"))
#Remove stop word flight
#customstopwords <- c("flight","airline","get","got","dont","will","ive","","told","day","still","can","cant")
#tweetCorpus<-tm_map(tweetCorpus,removeWords,customstopwords)


#tweetCorpus[[8]]$content
```


Create Term Document Matrix
```{r}
# Term Document Matrix
tdmtweetair<-TermDocumentMatrix(tweetCorpus)
inspect(tdmtweetair)
```

Convert to a Matrix
```{r}
# Convert to Matrix
mtweet<-as.matrix(tdmtweetair)
wordcount<-sort(rowSums(mtweet),decreasing = TRUE)
```

Check Word Frequency
```{r}
# Word Frequency
wordfrequency<-data.frame(text=names(wordcount),freq=wordcount)
head(wordfrequency)
```
### 5.4 Additional Visualizations

Plot Word Cloud
```{r}
# Word Cloud
wordfrequency<-data.frame(text=names(wordcount),freq=wordcount)
wordcloud(words =wordfrequency$text,freq=wordfrequency$freq,min.freq = 1,
          max.words=50,random.order = FALSE,rot.per=0.35,colors = brewer.pal(8,"Dark2"))
```
Plot Top 20 words from tweets
```{r}
# Top 20 words from the tweets

uniquewords<-wordfrequency%>%
  arrange(-freq)%>%
  top_n(20)
uniqueWordsPlot<-ggplot(uniquewords) +
  geom_col(
    mapping = aes(x = freq, y = text, fill = text), position = "dodge"
  )

uniqueWordsPlot
```
Find terms that appear at least a 100 times in the Term Document Matrix
```{r}
# Find terms appearing at least 100 times from TDM
findFreqTerms(tdmtweetair,100)
```


```{r}
dtm <- preprocessCorpus(tweetCorpus)
convertToDirection(analyzeSentiment(dtm)$SentimentQDAP)
```

Create Document Term Matrix
```{r}
# Create Document Term Matrix
dtmtweetairline<-DocumentTermMatrix(tweetCorpus)
```

Convert Sentiment to a Factor
```{r}
#Sentiment convert to factor
tweets$sentiment <- as.factor(tweets$sentiment)
```

```{r}

#PArtition index data 80-20

train_index <- sample(1:nrow(tweets), 0.8 * nrow(tweets))
test_index <- setdiff(1:nrow(tweets), train_index)


#Train and Test set for document matrix,corpus and original dataframe



doc.train <- dtmtweetairline[train_index,]    
doc.test <- dtmtweetairline[test_index,]  

corpus.train <-tweetCorpus[train_index] 
corpus.test <- tweetCorpus[test_index] 

tweets.train<-tweets[train_index,]
tweets.test<-tweets[test_index,]

```

```{r}
#Get terms atleast 5 times in document matrix

fivefreq <- findFreqTerms(doc.train, 5)

# Update the document matrix for frequent terms

dtm.train<- DocumentTermMatrix(corpus.train, control=list(dictionary = fivefreq))
dtm.test <- DocumentTermMatrix(corpus.test, control=list(dictionary = fivefreq))


#convert term frequency to boolean

convert_count <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}

train<-apply(dtm.train,2,convert_count)
test<-apply(dtm.test,2,convert_count)

dim(train)
dim(tweets.train)
```

```{r}

#Naives Bayes Classification

senticlassifier <- naiveBayes(train, tweets.train$sentiment, laplace = 1) 

pred<-predict(senticlassifier,newdata=test)
#table("Predictions"= pred,  "Actual" = tweets.test$sentiment )
```


# Bibliography

https://data.world/crowdflower/airline-twitter-sentiment

https://www.kaggle.com/crowdflower/twitter-airline-sentiment/data

https://appen.com/

https://github.com/benhamner/crowdflower-airline-twitter-sentiment/blob/master/src/process.py

https://gist.github.com/CateGitau/05e6ff80b2a3aaa58236067811cee44e)

